<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Understanding Transfer Learning With Pretrained VGG16 Network | Deshrit Baral</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Understanding Transfer Learning With Pretrained VGG16 Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transfer learning is a powerful machine learning technique where a trained models’ parameters are reused instead of training from scratch in order to fine tune on specific dataset for specialized problem." />
<meta property="og:description" content="Transfer learning is a powerful machine learning technique where a trained models’ parameters are reused instead of training from scratch in order to fine tune on specific dataset for specialized problem." />
<link rel="canonical" href="https://deshritbaral.com.np/blog/2025/05/06/transfer-learning" />
<meta property="og:url" content="https://deshritbaral.com.np/blog/2025/05/06/transfer-learning" />
<meta property="og:site_name" content="Deshrit Baral" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Understanding Transfer Learning With Pretrained VGG16 Network" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-05-06T00:00:00+00:00","datePublished":"2025-05-06T00:00:00+00:00","description":"Transfer learning is a powerful machine learning technique where a trained models’ parameters are reused instead of training from scratch in order to fine tune on specific dataset for specialized problem.","headline":"Understanding Transfer Learning With Pretrained VGG16 Network","mainEntityOfPage":{"@type":"WebPage","@id":"https://deshritbaral.com.np/blog/2025/05/06/transfer-learning"},"url":"https://deshritbaral.com.np/blog/2025/05/06/transfer-learning"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://deshritbaral.com.np/feed.xml" title="Deshrit Baral" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '{"id"=>"G-CMDTH72TR2"}', 'auto');
  ga('send', 'pageview');
}
</script>
  
<link rel="icon" href="/favicon.png" type="image/x-icon">

  <!-- Google Analytics -->
  
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CMDTH72TR2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag("js", new Date());
  gtag("config", "G-CMDTH72TR2");
</script>

  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Deshrit Baral</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/blog">Blog</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <p class="post-title p-name" itemprop="name headline">Understanding Transfer Learning With Pretrained VGG16 Network</p>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-05-06T00:00:00+00:00" itemprop="datePublished">May 6, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Transfer learning is a powerful machine learning technique where a trained models’ 
parameters are reused instead of training from scratch in order to fine tune on 
specific dataset for specialized problem.</p>

<p>In this blog, we will try to understand transfer learning through hands on experience 
with PyTroch framework using pretrained <a href="https://arxiv.org/abs/1409.1556">VGG16</a> network
trained on <a href="https://www.image-net.org/">ImageNet</a> dataset on to 
new <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765">Cat and Dogs</a> dataset.</p>

<hr />

<h2 id="why-transfer-learning">Why Transfer Learning?</h2>

<p>Training a large convolutional neural network from scratch reqiuires millions of 
labeled images, significant GPU compute and long training time. Instead, with this learning
technique we can simply load a pretrained model trained on generalized dataset like <a href="https://www.image-net.org/">ImageNet</a> (1.2 million images),
replace the final classification layer and train only the last few layers (or we can fine tune entire model).</p>

<hr />

<h2 id="about-vgg16">About VGG16</h2>

<p>VGG (Visual Geometry Group) was the name of the team participating in <a href="https://www.image-net.org/challenges/LSVRC/2014/index.php">ILSVRC - 2014</a>
from University of oxford, Department of Engineering Science. Their one of the purposed network 
architecture with 16 layers (convolutional and dense layers) was called VGG16. The 
original paper explaining everything in detail about VGG networks is available in
<a href="https://arxiv.org/abs/1409.1556">arxiv</a>.</p>

<p>The pretrained VGG16 model, as the name says, is 16 layers deep with 13 convolutional 
layers and 3 fully connected layers. It is trained on ImageNet dataset with 1,000 object
categories. The input image size to the model was 224 x 224 pixels. The model used 
3x3 convolutional filters throughout the network and ended with fully connected classifier
followed by a softmax layer for classification.</p>

<hr />

<h2 id="download-and-extract-dataset">Download and Extract Dataset</h2>

<p>Download the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765">Cat and Dogs</a> 
dataset and extract the zip file. You will get the dataset directory as <code class="language-plaintext highlighter-rouge">kagglecatsanddogs_5340</code> 
and inside cut <code class="language-plaintext highlighter-rouge">PetImages</code> directory and paste at the project root or if comfortable 
leave as is and update the path.</p>

<hr />

<h2 id="step-1-install-and-import-libraries">Step 1: Install and Import libraries</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">random_split</span>

<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">v2</span>
</code></pre></div></div>

<hr />

<h2 id="step-2-custom-dataset-class">Step 2: Custom Dataset Class</h2>

<p>In order to train a PyTorch model we should create compatible dataset and data loaders.
To learn more about them study <a href="https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CatDogDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root_dir</span> <span class="o">=</span> <span class="s">"PetImages"</span><span class="p">,</span> <span class="n">transforms</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span> 
        
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Cat"</span><span class="p">,</span> <span class="s">"Dog"</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">class_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="s">"Cat"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"Dog"</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">class_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="p">:</span>
            <span class="n">class_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="n">class_name</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">img_name</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">class_dir</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">img_name</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">endswith</span><span class="p">((</span><span class="s">".png"</span><span class="p">,</span> <span class="s">".jpg"</span><span class="p">,</span> <span class="s">".jpeg"</span><span class="p">)):</span>
                    <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">class_dir</span><span class="p">,</span> <span class="n">img_name</span><span class="p">)</span>

                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">with</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">img</span><span class="p">:</span>
                            <span class="n">img</span><span class="p">.</span><span class="n">verify</span><span class="p">()</span>
                        
                        <span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
                        <span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">class_to_id</span><span class="p">[</span><span class="n">class_name</span><span class="p">])</span>
                    
                    <span class="k">except</span> <span class="p">(</span><span class="nb">OSError</span><span class="p">,</span> <span class="n">UnidentifiedImageError</span><span class="p">):</span>
                        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Skipping corrupted image: </span><span class="si">{</span><span class="n">img_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img_path</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">"RGB"</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</code></pre></div></div>

<hr />

<h2 id="step-3-define-image-transformations">Step 3: Define Image Transformations</h2>

<p>Since VGG16 expects 224×224 RGB images, we resize and normalize using ImageNet 
statistics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
<span class="n">std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span>

<span class="n">transforms</span> <span class="o">=</span> <span class="n">v2</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">v2</span><span class="p">.</span><span class="n">PILToTensor</span><span class="p">(),</span>
    <span class="n">v2</span><span class="p">.</span><span class="n">ToDtype</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">v2</span><span class="p">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">v2</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">),</span>
<span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="step-4-create-dataset-and-dataloaders">Step 4: Create Dataset and DataLoaders</h2>

<p>We are splitting 80% of total data into training and rest 20% into testing set. Here,
I am using <code class="language-plaintext highlighter-rouge">BATCH_SIZE</code> of 16 but once comfortable you can play around with this 
parameter considering the GPU memory size to get better results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DATASET_ROOT</span> <span class="o">=</span> <span class="s">"/content/kagglecatsanddogs_5340/PetImages"</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">CatDogDataset</span><span class="p">(</span><span class="n">DATASET_ROOT</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span><span class="p">)</span>

<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Train dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Validation dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train dataset: 19998</span>
<span class="c"># Validation dataset: 5000</span>
</code></pre></div></div>

<hr />

<h2 id="step-5-load-pretrained-vgg16">Step 5: Load Pretrained VGG16</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">models</span><span class="p">.</span><span class="n">VGG16_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="step-6-freeze-feature-extractor-layers">Step 6: Freeze Feature Extractor Layers</h2>

<p>These are the layers of VGG16 network. And, its last classification layer has 1000
neurons for the categories of <a href="https://www.image-net.org/">ImageNet</a> challenge.</p>

<p><img src="/assets/img/vgg16-layers.png" alt="vgg16-layers" /></p>

<p><em>Source: https://arxiv.org/pdf/1409.1556</em></p>

<p>We freeze convolution layers so we only train the classifier, for us in the case of cats
and dogs classification should be 2.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>

<hr />

<h2 id="step-7-replace-final-classifier-layer">Step 7: Replace Final Classifier Layer</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_features</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">in_features</span>

<span class="n">model</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="step-8-define-loss-and-optimizer">Step 8: Define Loss and Optimizer</h2>

<p>We are using <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a>
criterion to compute the cross entropy loss between input logits and target with fixed
learning rate of 0.001.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span>
    <span class="n">model</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="step-9-training-loop">Step 9: Training Loop</h2>

<p>We will train the model for 10 epochs. I train this notebook in  google colab L4 GPU 
but free tier GPU should be work.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    
    <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
    
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">EPOCHS</span><span class="si">}</span><span class="s">] "</span>
          <span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> "</span>
          <span class="sa">f</span><span class="s">"Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>

<span class="c1"># Epoch [1/10] Loss: 0.8439 Train Acc: 99.61%
# Epoch [2/10] Loss: 0.9217 Train Acc: 99.63%
# Epoch [3/10] Loss: 0.5612 Train Acc: 99.74%
# Epoch [4/10] Loss: 0.4510 Train Acc: 99.78%
# Epoch [5/10] Loss: 0.4539 Train Acc: 99.80%
# Epoch [6/10] Loss: 0.5069 Train Acc: 99.80%
# Epoch [7/10] Loss: 0.4641 Train Acc: 99.79%
# Epoch [8/10] Loss: 0.5079 Train Acc: 99.81%
# Epoch [9/10] Loss: 0.5364 Train Acc: 99.82%
# Epoch [10/10] Loss: 0.4578 Train Acc: 99.81%
</span></code></pre></div></div>

<hr />

<h2 id="step-10-validation">Step 10: Validation</h2>

<p>We evaluate the model’s performance in the validation dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="n">val_acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Validation Accuracy: </span><span class="si">{</span><span class="n">val_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>

<span class="c1"># Validation Accuracy: 98.72%
</span></code></pre></div></div>

<hr />

<h2 id="fine-tuning-for-maximum-performance">Fine Tuning for Maximum Performance</h2>

<p>So far, we trained only the new classifier head, while keeping the convolutional 
backbone frozen. This is often sufficient for many tasks. However, when the target 
dataset is significantly different from ImageNet, fine-tuning deeper layers can 
substantially improve accuracy.</p>

<p>Fine-Tuning works because pre-trained networks learn generic low-level features
in early layers like edges, colors, textures and task specific high level features
in deeper layers like shape, object parts and semantic structures.</p>

<p>For cats vs dogs, the low-level features are transferable, but high-level features 
such as fur patterns, ear shapes, and facial geometry benefit from fine-tuning.</p>

<hr />

<h2 id="fine-tuning-strategy">Fine-Tuning Strategy</h2>

<p>Instead of unfreezing the entire network, we:</p>

<ul>
  <li>Freeze early layers</li>
  <li>Unfreeze only the deeper layers</li>
  <li>Train with a very small learning rate</li>
</ul>

<p>This avoids catastrophic forgetting, overfitting and training instability.</p>

<hr />

<h2 id="selective-layer-unfreezing">Selective Layer Unfreezing</h2>

<p>We unfreeze only the later convolution blocks of VGG16:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="mi">24</span><span class="p">:].</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div>

<p>Since we’re modifying pretrained weights, we reduce learning rate:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div></div>

  </div><a class="u-url" href="/blog/2025/05/06/transfer-learning" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <h4>CONTACT</h4>
        <ul class="contact-list">
          <li class="p-name"><li><a class="u-email" href="mailto:deshritbaral@gmail.com">deshritbaral@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2">
        <h4>SOCIAL</h4><ul class="social-media-list"><li><a href="https://github.com/deshrit"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">deshrit</span></a></li><li><a href="https://www.linkedin.com/in/deshrit"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">deshrit</span></a></li><li><a href="https://youtube.com/%40deshrit"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">@deshrit</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id=”MathJax-script” async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </body>

</html>
